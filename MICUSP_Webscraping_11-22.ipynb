{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping the Michigan Corpus of Upper-Level Student Papers (MICUSP)\n",
    "\n",
    "The Michigan Corpus of Upper-Level Student Papers is a public repository of advanced, high-scoring student work in 16 disciplines. This code allows researchers to scrape all 829 papers from MICUSP using BeautifulSoup and download them as text files labeled by Paper ID. \n",
    "\n",
    "Browse MICUSP here: https://elicorpora.info/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install packages\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import bleach\n",
    "import pandas as pd\n",
    "\n",
    "#Install certificate (of meeded)\n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "import os\n",
    "path = os.getcwd()\n",
    "os.chdir('/Users/megankane/Documents/Corpora/MICUSP/MICUSP_NEW')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use BeautifulSoup to scrape the text of a single paper in MICUSP\n",
    "\n",
    "#Assign text url to variable\n",
    "u = 'https://elicorpora.info/view?pid=BIO.G0.15.1' \n",
    "\n",
    "#Open the url\n",
    "uf = urllib.request.urlopen(u).read() \n",
    "\n",
    "#Turn url into a soup object\n",
    "su = bsoup(uf) \n",
    "\n",
    "#Inspect soup object\n",
    "su"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get paper metadata\n",
    "papers = pd.read_csv('https://elicorpora.info/browse?mode=download&start=1&sort=dept&direction=asc')\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add relevant metadata to lists\n",
    "#We're most interested in labeling papers by ID, but can retrieve by discipline, type, or title\n",
    "pids = papers['PAPER ID'].tolist()\n",
    "\n",
    "#papertype = papers['PAPER TYPE'].tolist()\n",
    "#discipline = papers['DISCIPLINE'].tolist()\n",
    "#titles = papers['TITLE'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set url base to scrape multiple files\n",
    "url_base =  'https://elicorpora.info/view?pid='\n",
    "\n",
    "#Get urls of all text and add to list\n",
    "urls = []\n",
    "for p in pids:\n",
    "  urls.append(url_base + p)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through each text page and scrape with beautifulsoup\n",
    "files = []\n",
    "for item in urls:\n",
    "  files.append(urllib.request.urlopen(item).read())\n",
    "\n",
    "soups = []\n",
    "for f in files:\n",
    "  soups.append(bsoup(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check length of soups (should be 829)\n",
    "len(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolate paragraphs of each essay (do not want headers, bibliographies, other metadata)\n",
    "paragraphs = []\n",
    "for soup in soups:\n",
    "  paragraphs.append(soup.findAll('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to get the text of each paragraph and join together\n",
    "def get_paragraph_text(text):\n",
    "  ptexts = []\n",
    "  for p in text:\n",
    "    ptexts.append(p.getText())\n",
    "    \n",
    "  return ' '.join(ptexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add joined paragraphs of each text to a new list called essays\n",
    "essays = []\n",
    "for p in paragraphs:\n",
    "  essays.append(get_paragraph_text(p))\n",
    "\n",
    "len(essays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write each essay to file named by paper ID\n",
    "n = 0\n",
    "for i in essays:\n",
    "    f = open(pids[0] + \".txt\",'w')\n",
    "    n += 1\n",
    "    f.write(i)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
